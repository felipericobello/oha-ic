{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        df = pd.read_pickle(filename)\n",
    "        dfx = df.drop(columns=['X'])\n",
    "        dfy = df.drop(columns=['FAI', 'PA', 'A', 'Phase', 'DeltaTime'])\n",
    "        \n",
    "        x = dfx.iloc[:,:].values\n",
    "        y = dfy.iloc[:, :].values\n",
    "\n",
    "        x = np.array(x, dtype=np.float32)\n",
    "        self.X = torch.from_numpy(x)\n",
    "        self.X = torch.squeeze(self.X)\n",
    "\n",
    "        print(f'{filename} X size: {self.X.shape}')\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(len(y)):\n",
    "            outputs.append(y[i, :])\n",
    "            \n",
    "        self.y = torch.tensor(outputs, dtype=torch.float32)\n",
    "        self.y = torch.squeeze(self.y)\n",
    "        self.y = torch.squeeze(self.y)\n",
    "        print(f'{filename} Y size: {self.y.shape}')\n",
    "        \n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getparams__(self):\n",
    "        return self.X, self.y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-data-oha.pkl X size: torch.Size([5000, 5])\n",
      "train-data-oha.pkl Y size: torch.Size([5000, 300])\n",
      "test-data-oha.pkl X size: torch.Size([5000, 5])\n",
      "test-data-oha.pkl Y size: torch.Size([5000, 300])\n",
      "Shape of X [N, Params]: torch.Size([1, 5])\n",
      "Shape of y [N, Params]: torch.Size([1, 300]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_data = Data(filename='train-data-oha.pkl')\n",
    "test_data = Data(filename='test-data-oha.pkl')\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, shuffle=True)\n",
    "\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(f\"Shape of X [N, Params]: {X.shape}\")\n",
    "    test_x = X\n",
    "    print(f\"Shape of y [N, Params]: {y.shape} {y.dtype}\")\n",
    "    test_y = y\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=300, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(5, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 300)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 4.842700  [    0/ 5000]\n",
      "loss: 0.676385  [  100/ 5000]\n",
      "loss: 0.014150  [  200/ 5000]\n",
      "loss: 4.270140  [  300/ 5000]\n",
      "loss: 0.996626  [  400/ 5000]\n",
      "loss: 0.266973  [  500/ 5000]\n",
      "loss: 3.164845  [  600/ 5000]\n",
      "loss: 0.005874  [  700/ 5000]\n",
      "loss: 6.415883  [  800/ 5000]\n",
      "loss: 1.971574  [  900/ 5000]\n",
      "loss: 0.514689  [ 1000/ 5000]\n",
      "loss: 0.929281  [ 1100/ 5000]\n",
      "loss: 0.442867  [ 1200/ 5000]\n",
      "loss: 0.570437  [ 1300/ 5000]\n",
      "loss: 0.026146  [ 1400/ 5000]\n",
      "loss: 0.034483  [ 1500/ 5000]\n",
      "loss: 0.176362  [ 1600/ 5000]\n",
      "loss: 0.588350  [ 1700/ 5000]\n",
      "loss: 1.277087  [ 1800/ 5000]\n",
      "loss: 0.069926  [ 1900/ 5000]\n",
      "loss: 2.199167  [ 2000/ 5000]\n",
      "loss: 6.751662  [ 2100/ 5000]\n",
      "loss: 57.948269  [ 2200/ 5000]\n",
      "loss: 0.906282  [ 2300/ 5000]\n",
      "loss: 0.017943  [ 2400/ 5000]\n",
      "loss: 0.106388  [ 2500/ 5000]\n",
      "loss: 2.277515  [ 2600/ 5000]\n",
      "loss: 29.626686  [ 2700/ 5000]\n",
      "loss: 147.796005  [ 2800/ 5000]\n",
      "loss: 10.953046  [ 2900/ 5000]\n",
      "loss: 18.770172  [ 3000/ 5000]\n",
      "loss: 0.486589  [ 3100/ 5000]\n",
      "loss: 0.170510  [ 3200/ 5000]\n",
      "loss: 0.100435  [ 3300/ 5000]\n",
      "loss: 0.141353  [ 3400/ 5000]\n",
      "loss: 9.688741  [ 3500/ 5000]\n",
      "loss: 0.318795  [ 3600/ 5000]\n",
      "loss: 0.073455  [ 3700/ 5000]\n",
      "loss: 1.708753  [ 3800/ 5000]\n",
      "loss: 0.882981  [ 3900/ 5000]\n",
      "loss: 5.442863  [ 4000/ 5000]\n",
      "loss: 1.565592  [ 4100/ 5000]\n",
      "loss: 2.137005  [ 4200/ 5000]\n",
      "loss: 9.478590  [ 4300/ 5000]\n",
      "loss: 2.502111  [ 4400/ 5000]\n",
      "loss: 0.656680  [ 4500/ 5000]\n",
      "loss: 0.018603  [ 4600/ 5000]\n",
      "loss: 2.864961  [ 4700/ 5000]\n",
      "loss: 0.111385  [ 4800/ 5000]\n",
      "loss: 1.222108  [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 2409.2%, Avg loss: 4.379590 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.179833  [    0/ 5000]\n",
      "loss: 0.262196  [  100/ 5000]\n",
      "loss: 0.698763  [  200/ 5000]\n",
      "loss: 0.682103  [  300/ 5000]\n",
      "loss: 1.145914  [  400/ 5000]\n",
      "loss: 0.021361  [  500/ 5000]\n",
      "loss: 8.777923  [  600/ 5000]\n",
      "loss: 3.924368  [  700/ 5000]\n",
      "loss: 0.169894  [  800/ 5000]\n",
      "loss: 3.839168  [  900/ 5000]\n",
      "loss: 4.524682  [ 1000/ 5000]\n",
      "loss: 24.221184  [ 1100/ 5000]\n",
      "loss: 0.042236  [ 1200/ 5000]\n",
      "loss: 1.652758  [ 1300/ 5000]\n",
      "loss: 0.058348  [ 1400/ 5000]\n",
      "loss: 19.558506  [ 1500/ 5000]\n",
      "loss: 2.071233  [ 1600/ 5000]\n",
      "loss: 2.477608  [ 1700/ 5000]\n",
      "loss: 0.006402  [ 1800/ 5000]\n",
      "loss: 0.280998  [ 1900/ 5000]\n",
      "loss: 0.654500  [ 2000/ 5000]\n",
      "loss: 82.747902  [ 2100/ 5000]\n",
      "loss: 0.403733  [ 2200/ 5000]\n",
      "loss: 19.059843  [ 2300/ 5000]\n",
      "loss: 0.037428  [ 2400/ 5000]\n",
      "loss: 3.170159  [ 2500/ 5000]\n",
      "loss: 0.120902  [ 2600/ 5000]\n",
      "loss: 0.663300  [ 2700/ 5000]\n",
      "loss: 0.061037  [ 2800/ 5000]\n",
      "loss: 2.129077  [ 2900/ 5000]\n",
      "loss: 0.910332  [ 3000/ 5000]\n",
      "loss: 4.831670  [ 3100/ 5000]\n",
      "loss: 0.015975  [ 3200/ 5000]\n",
      "loss: 1.539155  [ 3300/ 5000]\n",
      "loss: 1.355549  [ 3400/ 5000]\n",
      "loss: 1.134158  [ 3500/ 5000]\n",
      "loss: 0.286615  [ 3600/ 5000]\n",
      "loss: 0.678186  [ 3700/ 5000]\n",
      "loss: 0.184075  [ 3800/ 5000]\n",
      "loss: 10.450854  [ 3900/ 5000]\n",
      "loss: 1.271173  [ 4000/ 5000]\n",
      "loss: 123.478699  [ 4100/ 5000]\n",
      "loss: 1.500469  [ 4200/ 5000]\n",
      "loss: 1.995535  [ 4300/ 5000]\n",
      "loss: 0.401023  [ 4400/ 5000]\n",
      "loss: 7.103710  [ 4500/ 5000]\n",
      "loss: 0.163029  [ 4600/ 5000]\n",
      "loss: 0.578853  [ 4700/ 5000]\n",
      "loss: 0.844289  [ 4800/ 5000]\n",
      "loss: 0.746507  [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 2409.2%, Avg loss: 4.074384 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.163142  [    0/ 5000]\n",
      "loss: 0.231767  [  100/ 5000]\n",
      "loss: 0.578407  [  200/ 5000]\n",
      "loss: 0.484666  [  300/ 5000]\n",
      "loss: 1.794005  [  400/ 5000]\n",
      "loss: 0.180326  [  500/ 5000]\n",
      "loss: 0.507656  [  600/ 5000]\n",
      "loss: 0.078910  [  700/ 5000]\n",
      "loss: 9.451209  [  800/ 5000]\n",
      "loss: 20.692156  [  900/ 5000]\n",
      "loss: 1.461005  [ 1000/ 5000]\n",
      "loss: 0.400235  [ 1100/ 5000]\n",
      "loss: 0.531733  [ 1200/ 5000]\n",
      "loss: 0.433066  [ 1300/ 5000]\n",
      "loss: 0.132613  [ 1400/ 5000]\n",
      "loss: 0.484796  [ 1500/ 5000]\n",
      "loss: 14.561894  [ 1600/ 5000]\n",
      "loss: 0.044520  [ 1700/ 5000]\n",
      "loss: 3.116878  [ 1800/ 5000]\n",
      "loss: 0.143121  [ 1900/ 5000]\n",
      "loss: 0.487373  [ 2000/ 5000]\n",
      "loss: 0.298225  [ 2100/ 5000]\n",
      "loss: 0.783711  [ 2200/ 5000]\n",
      "loss: 1.486379  [ 2300/ 5000]\n",
      "loss: 13.849998  [ 2400/ 5000]\n",
      "loss: 1.489080  [ 2500/ 5000]\n",
      "loss: 0.255543  [ 2600/ 5000]\n",
      "loss: 9.670054  [ 2700/ 5000]\n",
      "loss: 8.028765  [ 2800/ 5000]\n",
      "loss: 0.876319  [ 2900/ 5000]\n",
      "loss: 2.985800  [ 3000/ 5000]\n",
      "loss: 0.194235  [ 3100/ 5000]\n",
      "loss: 0.188686  [ 3200/ 5000]\n",
      "loss: 4.630605  [ 3300/ 5000]\n",
      "loss: 0.489301  [ 3400/ 5000]\n",
      "loss: 0.601708  [ 3500/ 5000]\n",
      "loss: 1.016475  [ 3600/ 5000]\n",
      "loss: 1.404499  [ 3700/ 5000]\n",
      "loss: 105.457321  [ 3800/ 5000]\n",
      "loss: 0.277232  [ 3900/ 5000]\n",
      "loss: 0.271205  [ 4000/ 5000]\n",
      "loss: 2.057045  [ 4100/ 5000]\n",
      "loss: 0.216054  [ 4200/ 5000]\n",
      "loss: 0.612747  [ 4300/ 5000]\n",
      "loss: 2.526003  [ 4400/ 5000]\n",
      "loss: 2.716181  [ 4500/ 5000]\n",
      "loss: 0.287804  [ 4600/ 5000]\n",
      "loss: 1.409967  [ 4700/ 5000]\n",
      "loss: 6.628639  [ 4800/ 5000]\n",
      "loss: 0.130729  [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 2409.2%, Avg loss: 3.883122 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.369550  [    0/ 5000]\n",
      "loss: 0.707336  [  100/ 5000]\n",
      "loss: 0.545021  [  200/ 5000]\n",
      "loss: 2.607053  [  300/ 5000]\n",
      "loss: 0.175102  [  400/ 5000]\n",
      "loss: 0.807050  [  500/ 5000]\n",
      "loss: 4.136212  [  600/ 5000]\n",
      "loss: 0.353356  [  700/ 5000]\n",
      "loss: 0.767436  [  800/ 5000]\n",
      "loss: 0.228422  [  900/ 5000]\n",
      "loss: 0.980236  [ 1000/ 5000]\n",
      "loss: 0.336341  [ 1100/ 5000]\n",
      "loss: 2.453674  [ 1200/ 5000]\n",
      "loss: 34.205280  [ 1300/ 5000]\n",
      "loss: 0.015667  [ 1400/ 5000]\n",
      "loss: 3.999106  [ 1500/ 5000]\n",
      "loss: 9.292648  [ 1600/ 5000]\n",
      "loss: 0.783134  [ 1700/ 5000]\n",
      "loss: 1.671431  [ 1800/ 5000]\n",
      "loss: 0.729353  [ 1900/ 5000]\n",
      "loss: 0.021834  [ 2000/ 5000]\n",
      "loss: 0.039358  [ 2100/ 5000]\n",
      "loss: 0.049714  [ 2200/ 5000]\n",
      "loss: 0.281369  [ 2300/ 5000]\n",
      "loss: 0.024513  [ 2400/ 5000]\n",
      "loss: 0.027564  [ 2500/ 5000]\n",
      "loss: 2.207254  [ 2600/ 5000]\n",
      "loss: 0.043855  [ 2700/ 5000]\n",
      "loss: 11.470368  [ 2800/ 5000]\n",
      "loss: 0.284088  [ 2900/ 5000]\n",
      "loss: 0.186228  [ 3000/ 5000]\n",
      "loss: 1.744505  [ 3100/ 5000]\n",
      "loss: 4.991086  [ 3200/ 5000]\n",
      "loss: 2.804284  [ 3300/ 5000]\n",
      "loss: 1.800339  [ 3400/ 5000]\n",
      "loss: 0.156618  [ 3500/ 5000]\n",
      "loss: 4.636133  [ 3600/ 5000]\n",
      "loss: 0.182067  [ 3700/ 5000]\n",
      "loss: 0.336170  [ 3800/ 5000]\n",
      "loss: 1.146083  [ 3900/ 5000]\n",
      "loss: 0.202201  [ 4000/ 5000]\n",
      "loss: 0.044638  [ 4100/ 5000]\n",
      "loss: 5.565872  [ 4200/ 5000]\n",
      "loss: 84.280487  [ 4300/ 5000]\n",
      "loss: 0.403954  [ 4400/ 5000]\n",
      "loss: 0.207316  [ 4500/ 5000]\n",
      "loss: 0.177755  [ 4600/ 5000]\n",
      "loss: 0.844519  [ 4700/ 5000]\n",
      "loss: 0.864086  [ 4800/ 5000]\n",
      "loss: 0.286337  [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 2409.1%, Avg loss: 3.796645 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.999840  [    0/ 5000]\n",
      "loss: 0.643712  [  100/ 5000]\n",
      "loss: 0.135232  [  200/ 5000]\n",
      "loss: 4.284676  [  300/ 5000]\n",
      "loss: 0.239963  [  400/ 5000]\n",
      "loss: 0.162964  [  500/ 5000]\n",
      "loss: 3.720623  [  600/ 5000]\n",
      "loss: 0.028444  [  700/ 5000]\n",
      "loss: 128.956146  [  800/ 5000]\n",
      "loss: 0.661124  [  900/ 5000]\n",
      "loss: 0.487519  [ 1000/ 5000]\n",
      "loss: 0.226387  [ 1100/ 5000]\n",
      "loss: 0.062894  [ 1200/ 5000]\n",
      "loss: 0.348285  [ 1300/ 5000]\n",
      "loss: 0.719765  [ 1400/ 5000]\n",
      "loss: 18.342882  [ 1500/ 5000]\n",
      "loss: 0.069510  [ 1600/ 5000]\n",
      "loss: 0.101354  [ 1700/ 5000]\n",
      "loss: 2.998916  [ 1800/ 5000]\n",
      "loss: 0.228255  [ 1900/ 5000]\n",
      "loss: 1.358814  [ 2000/ 5000]\n",
      "loss: 0.144960  [ 2100/ 5000]\n",
      "loss: 11.186703  [ 2200/ 5000]\n",
      "loss: 4.388080  [ 2300/ 5000]\n",
      "loss: 53.255535  [ 2400/ 5000]\n",
      "loss: 0.059464  [ 2500/ 5000]\n",
      "loss: 0.029246  [ 2600/ 5000]\n",
      "loss: 1.731872  [ 2700/ 5000]\n",
      "loss: 3.744366  [ 2800/ 5000]\n",
      "loss: 0.087711  [ 2900/ 5000]\n",
      "loss: 0.183873  [ 3000/ 5000]\n",
      "loss: 4.184614  [ 3100/ 5000]\n",
      "loss: 7.829514  [ 3200/ 5000]\n",
      "loss: 0.938095  [ 3300/ 5000]\n",
      "loss: 0.006522  [ 3400/ 5000]\n",
      "loss: 2.061420  [ 3500/ 5000]\n",
      "loss: 1.828966  [ 3600/ 5000]\n",
      "loss: 1.473444  [ 3700/ 5000]\n",
      "loss: 0.383947  [ 3800/ 5000]\n",
      "loss: 1.005701  [ 3900/ 5000]\n",
      "loss: 0.028656  [ 4000/ 5000]\n",
      "loss: 0.218713  [ 4100/ 5000]\n",
      "loss: 2.541521  [ 4200/ 5000]\n",
      "loss: 0.275601  [ 4300/ 5000]\n",
      "loss: 0.350745  [ 4400/ 5000]\n",
      "loss: 0.504525  [ 4500/ 5000]\n",
      "loss: 54.842396  [ 4600/ 5000]\n",
      "loss: 2.851677  [ 4700/ 5000]\n",
      "loss: 1.082418  [ 4800/ 5000]\n",
      "loss: 0.425154  [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 2405.1%, Avg loss: 3.707100 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.546628  [    0/ 5000]\n",
      "loss: 1.954124  [  100/ 5000]\n",
      "loss: 0.074245  [  200/ 5000]\n",
      "loss: 0.834282  [  300/ 5000]\n",
      "loss: 0.031418  [  400/ 5000]\n",
      "loss: 0.844371  [  500/ 5000]\n",
      "loss: 1.028726  [  600/ 5000]\n",
      "loss: 54.698261  [  700/ 5000]\n",
      "loss: 0.013269  [  800/ 5000]\n",
      "loss: 3.441343  [  900/ 5000]\n",
      "loss: 9.562118  [ 1000/ 5000]\n",
      "loss: 0.088251  [ 1100/ 5000]\n",
      "loss: 0.552841  [ 1200/ 5000]\n",
      "loss: 15.645228  [ 1300/ 5000]\n",
      "loss: 0.689271  [ 1400/ 5000]\n",
      "loss: 20.549818  [ 1500/ 5000]\n",
      "loss: 0.290887  [ 1600/ 5000]\n",
      "loss: 0.316980  [ 1700/ 5000]\n",
      "loss: 0.323002  [ 1800/ 5000]\n",
      "loss: 0.147251  [ 1900/ 5000]\n",
      "loss: 1.030307  [ 2000/ 5000]\n",
      "loss: 1.074139  [ 2100/ 5000]\n",
      "loss: 2.363828  [ 2200/ 5000]\n",
      "loss: 0.144007  [ 2300/ 5000]\n",
      "loss: 0.521734  [ 2400/ 5000]\n",
      "loss: 0.274011  [ 2500/ 5000]\n",
      "loss: 0.169638  [ 2600/ 5000]\n",
      "loss: 0.078171  [ 2700/ 5000]\n",
      "loss: 10.550709  [ 2800/ 5000]\n",
      "loss: 9.694296  [ 2900/ 5000]\n",
      "loss: 16.984240  [ 3000/ 5000]\n",
      "loss: 0.190070  [ 3100/ 5000]\n",
      "loss: 0.143205  [ 3200/ 5000]\n",
      "loss: 0.630550  [ 3300/ 5000]\n",
      "loss: 0.459857  [ 3400/ 5000]\n",
      "loss: 2.455034  [ 3500/ 5000]\n",
      "loss: 1.149652  [ 3600/ 5000]\n",
      "loss: 0.006402  [ 3700/ 5000]\n",
      "loss: 0.192102  [ 3800/ 5000]\n",
      "loss: 2.511933  [ 3900/ 5000]\n",
      "loss: 0.243476  [ 4000/ 5000]\n",
      "loss: 0.051662  [ 4100/ 5000]\n",
      "loss: 0.541037  [ 4200/ 5000]\n",
      "loss: 0.158026  [ 4300/ 5000]\n",
      "loss: 0.301150  [ 4400/ 5000]\n",
      "loss: 0.461921  [ 4500/ 5000]\n",
      "loss: 0.456235  [ 4600/ 5000]\n",
      "loss: 0.283981  [ 4700/ 5000]\n",
      "loss: 0.109355  [ 4800/ 5000]\n",
      "loss: 3.598288  [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 2388.6%, Avg loss: 3.574924 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.101442  [    0/ 5000]\n",
      "loss: 8.172783  [  100/ 5000]\n",
      "loss: 0.226728  [  200/ 5000]\n",
      "loss: 0.239584  [  300/ 5000]\n",
      "loss: 0.268307  [  400/ 5000]\n",
      "loss: 0.693513  [  500/ 5000]\n",
      "loss: 0.231538  [  600/ 5000]\n",
      "loss: 0.557918  [  700/ 5000]\n",
      "loss: 0.519406  [  800/ 5000]\n",
      "loss: 1.076498  [  900/ 5000]\n",
      "loss: 0.081691  [ 1000/ 5000]\n",
      "loss: 1.688235  [ 1100/ 5000]\n",
      "loss: 0.049004  [ 1200/ 5000]\n",
      "loss: 1.235885  [ 1300/ 5000]\n",
      "loss: 17.075518  [ 1400/ 5000]\n",
      "loss: 3.402869  [ 1500/ 5000]\n",
      "loss: 2.095882  [ 1600/ 5000]\n",
      "loss: 0.045310  [ 1700/ 5000]\n",
      "loss: 0.026658  [ 1800/ 5000]\n",
      "loss: 0.498822  [ 1900/ 5000]\n",
      "loss: 0.061659  [ 2000/ 5000]\n",
      "loss: 2.442146  [ 2100/ 5000]\n",
      "loss: 0.611968  [ 2200/ 5000]\n",
      "loss: 2.878525  [ 2300/ 5000]\n",
      "loss: 5.328332  [ 2400/ 5000]\n",
      "loss: 0.793155  [ 2500/ 5000]\n",
      "loss: 0.389408  [ 2600/ 5000]\n",
      "loss: 0.025840  [ 2700/ 5000]\n",
      "loss: 0.035619  [ 2800/ 5000]\n",
      "loss: 0.046936  [ 2900/ 5000]\n",
      "loss: 0.278395  [ 3000/ 5000]\n",
      "loss: 0.382872  [ 3100/ 5000]\n",
      "loss: 0.044219  [ 3200/ 5000]\n",
      "loss: 4.630640  [ 3300/ 5000]\n",
      "loss: 70.201073  [ 3400/ 5000]\n",
      "loss: 0.154898  [ 3500/ 5000]\n",
      "loss: 0.324105  [ 3600/ 5000]\n",
      "loss: 30.720049  [ 3700/ 5000]\n",
      "loss: 4.776397  [ 3800/ 5000]\n",
      "loss: 0.974377  [ 3900/ 5000]\n",
      "loss: 0.263209  [ 4000/ 5000]\n",
      "loss: 0.101370  [ 4100/ 5000]\n",
      "loss: 4.610665  [ 4200/ 5000]\n",
      "loss: 14.441217  [ 4300/ 5000]\n",
      "loss: 0.082961  [ 4400/ 5000]\n",
      "loss: 0.287436  [ 4500/ 5000]\n",
      "loss: 3.593427  [ 4600/ 5000]\n",
      "loss: 0.479151  [ 4700/ 5000]\n",
      "loss: 0.182785  [ 4800/ 5000]\n",
      "loss: 2.991807  [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 2374.1%, Avg loss: 3.481482 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.338900  [    0/ 5000]\n",
      "loss: 0.821751  [  100/ 5000]\n",
      "loss: 0.624315  [  200/ 5000]\n",
      "loss: 0.926302  [  300/ 5000]\n",
      "loss: 0.270549  [  400/ 5000]\n",
      "loss: 1.138021  [  500/ 5000]\n",
      "loss: 0.208386  [  600/ 5000]\n",
      "loss: 0.065603  [  700/ 5000]\n",
      "loss: 35.655262  [  800/ 5000]\n",
      "loss: 7.354990  [  900/ 5000]\n",
      "loss: 0.738223  [ 1000/ 5000]\n",
      "loss: 0.579499  [ 1100/ 5000]\n",
      "loss: 0.976733  [ 1200/ 5000]\n",
      "loss: 0.395918  [ 1300/ 5000]\n",
      "loss: 115.631943  [ 1400/ 5000]\n",
      "loss: 0.204235  [ 1500/ 5000]\n",
      "loss: 0.115726  [ 1600/ 5000]\n",
      "loss: 0.563348  [ 1700/ 5000]\n",
      "loss: 1.405420  [ 1800/ 5000]\n",
      "loss: 5.567982  [ 1900/ 5000]\n",
      "loss: 0.191197  [ 2000/ 5000]\n",
      "loss: 0.373131  [ 2100/ 5000]\n",
      "loss: 0.230983  [ 2200/ 5000]\n",
      "loss: 3.685979  [ 2300/ 5000]\n",
      "loss: 16.443460  [ 2400/ 5000]\n",
      "loss: 39.670151  [ 2500/ 5000]\n",
      "loss: 0.315267  [ 2600/ 5000]\n",
      "loss: 0.062820  [ 2700/ 5000]\n",
      "loss: 0.028883  [ 2800/ 5000]\n",
      "loss: 0.108498  [ 2900/ 5000]\n",
      "loss: 0.477847  [ 3000/ 5000]\n",
      "loss: 0.237039  [ 3100/ 5000]\n",
      "loss: 0.839286  [ 3200/ 5000]\n",
      "loss: 0.168649  [ 3300/ 5000]\n",
      "loss: 4.053797  [ 3400/ 5000]\n",
      "loss: 0.081638  [ 3500/ 5000]\n",
      "loss: 0.076018  [ 3600/ 5000]\n",
      "loss: 1.849798  [ 3700/ 5000]\n",
      "loss: 3.650250  [ 3800/ 5000]\n",
      "loss: 2.649925  [ 3900/ 5000]\n",
      "loss: 1.246764  [ 4000/ 5000]\n",
      "loss: 0.047480  [ 4100/ 5000]\n",
      "loss: 0.222595  [ 4200/ 5000]\n",
      "loss: 0.917826  [ 4300/ 5000]\n",
      "loss: 0.014026  [ 4400/ 5000]\n",
      "loss: 0.132311  [ 4500/ 5000]\n",
      "loss: 0.317398  [ 4600/ 5000]\n",
      "loss: 1.116773  [ 4700/ 5000]\n",
      "loss: 0.375291  [ 4800/ 5000]\n",
      "loss: 0.453283  [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 2333.5%, Avg loss: 3.434349 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.060810  [    0/ 5000]\n",
      "loss: 1.995342  [  100/ 5000]\n",
      "loss: 40.080093  [  200/ 5000]\n",
      "loss: 3.116965  [  300/ 5000]\n",
      "loss: 0.045589  [  400/ 5000]\n",
      "loss: 0.148987  [  500/ 5000]\n",
      "loss: 1.981613  [  600/ 5000]\n",
      "loss: 2.737994  [  700/ 5000]\n",
      "loss: 0.637861  [  800/ 5000]\n",
      "loss: 1.523888  [  900/ 5000]\n",
      "loss: 0.025369  [ 1000/ 5000]\n",
      "loss: 0.174462  [ 1100/ 5000]\n",
      "loss: 2.731432  [ 1200/ 5000]\n",
      "loss: 0.974922  [ 1300/ 5000]\n",
      "loss: 2.754660  [ 1400/ 5000]\n",
      "loss: 0.067549  [ 1500/ 5000]\n",
      "loss: 0.069439  [ 1600/ 5000]\n",
      "loss: 0.107770  [ 1700/ 5000]\n",
      "loss: 0.779373  [ 1800/ 5000]\n",
      "loss: 0.356841  [ 1900/ 5000]\n",
      "loss: 1.598489  [ 2000/ 5000]\n",
      "loss: 0.060821  [ 2100/ 5000]\n",
      "loss: 2.429938  [ 2200/ 5000]\n",
      "loss: 0.202577  [ 2300/ 5000]\n",
      "loss: 13.941017  [ 2400/ 5000]\n",
      "loss: 0.222185  [ 2500/ 5000]\n",
      "loss: 0.689793  [ 2600/ 5000]\n",
      "loss: 1.133198  [ 2700/ 5000]\n",
      "loss: 0.678286  [ 2800/ 5000]\n",
      "loss: 1.809977  [ 2900/ 5000]\n",
      "loss: 0.376597  [ 3000/ 5000]\n",
      "loss: 0.228802  [ 3100/ 5000]\n",
      "loss: 0.636821  [ 3200/ 5000]\n",
      "loss: 0.170268  [ 3300/ 5000]\n",
      "loss: 3.505938  [ 3400/ 5000]\n",
      "loss: 1.048124  [ 3500/ 5000]\n",
      "loss: 0.691739  [ 3600/ 5000]\n",
      "loss: 0.101947  [ 3700/ 5000]\n",
      "loss: 0.636700  [ 3800/ 5000]\n",
      "loss: 0.399778  [ 3900/ 5000]\n",
      "loss: 0.189227  [ 4000/ 5000]\n",
      "loss: 0.114614  [ 4100/ 5000]\n",
      "loss: 1.659364  [ 4200/ 5000]\n",
      "loss: 0.534145  [ 4300/ 5000]\n",
      "loss: 1.007557  [ 4400/ 5000]\n",
      "loss: 0.013282  [ 4500/ 5000]\n",
      "loss: 0.354866  [ 4600/ 5000]\n",
      "loss: 28.237797  [ 4700/ 5000]\n",
      "loss: 19.569412  [ 4800/ 5000]\n",
      "loss: 12.461458  [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 2316.6%, Avg loss: 3.360000 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.423370  [    0/ 5000]\n",
      "loss: 0.132977  [  100/ 5000]\n",
      "loss: 0.845286  [  200/ 5000]\n",
      "loss: 0.110822  [  300/ 5000]\n",
      "loss: 14.909457  [  400/ 5000]\n",
      "loss: 2.451944  [  500/ 5000]\n",
      "loss: 1.293369  [  600/ 5000]\n",
      "loss: 1.818308  [  700/ 5000]\n",
      "loss: 0.196758  [  800/ 5000]\n",
      "loss: 0.796748  [  900/ 5000]\n",
      "loss: 0.324386  [ 1000/ 5000]\n",
      "loss: 0.156483  [ 1100/ 5000]\n",
      "loss: 0.339189  [ 1200/ 5000]\n",
      "loss: 0.237464  [ 1300/ 5000]\n",
      "loss: 9.957499  [ 1400/ 5000]\n",
      "loss: 2.210676  [ 1500/ 5000]\n",
      "loss: 0.114761  [ 1600/ 5000]\n",
      "loss: 0.322984  [ 1700/ 5000]\n",
      "loss: 63.121017  [ 1800/ 5000]\n",
      "loss: 0.806710  [ 1900/ 5000]\n",
      "loss: 1.300109  [ 2000/ 5000]\n",
      "loss: 0.097737  [ 2100/ 5000]\n",
      "loss: 0.153200  [ 2200/ 5000]\n",
      "loss: 0.081712  [ 2300/ 5000]\n",
      "loss: 0.028846  [ 2400/ 5000]\n",
      "loss: 0.616002  [ 2500/ 5000]\n",
      "loss: 1.226702  [ 2600/ 5000]\n",
      "loss: 0.313473  [ 2700/ 5000]\n",
      "loss: 1.246454  [ 2800/ 5000]\n",
      "loss: 0.350481  [ 2900/ 5000]\n",
      "loss: 0.660155  [ 3000/ 5000]\n",
      "loss: 0.141179  [ 3100/ 5000]\n",
      "loss: 0.211555  [ 3200/ 5000]\n",
      "loss: 0.567572  [ 3300/ 5000]\n",
      "loss: 0.137972  [ 3400/ 5000]\n",
      "loss: 0.299681  [ 3500/ 5000]\n",
      "loss: 0.086187  [ 3600/ 5000]\n",
      "loss: 0.325123  [ 3700/ 5000]\n",
      "loss: 1.380600  [ 3800/ 5000]\n",
      "loss: 0.288919  [ 3900/ 5000]\n",
      "loss: 9.889015  [ 4000/ 5000]\n",
      "loss: 0.536820  [ 4100/ 5000]\n",
      "loss: 0.040198  [ 4200/ 5000]\n",
      "loss: 0.227687  [ 4300/ 5000]\n",
      "loss: 5.397411  [ 4400/ 5000]\n",
      "loss: 0.312531  [ 4500/ 5000]\n",
      "loss: 0.132295  [ 4600/ 5000]\n",
      "loss: 0.884481  [ 4700/ 5000]\n",
      "loss: 0.345882  [ 4800/ 5000]\n",
      "loss: 0.570697  [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 2322.0%, Avg loss: 3.312028 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.944085  [    0/ 5000]\n",
      "loss: 0.292071  [  100/ 5000]\n",
      "loss: 0.360834  [  200/ 5000]\n",
      "loss: 0.092636  [  300/ 5000]\n",
      "loss: 0.390636  [  400/ 5000]\n",
      "loss: 0.170064  [  500/ 5000]\n",
      "loss: 0.536852  [  600/ 5000]\n",
      "loss: 22.013708  [  700/ 5000]\n",
      "loss: 0.029637  [  800/ 5000]\n",
      "loss: 2.933650  [  900/ 5000]\n",
      "loss: 0.549199  [ 1000/ 5000]\n",
      "loss: 0.188477  [ 1100/ 5000]\n",
      "loss: 0.276092  [ 1200/ 5000]\n",
      "loss: 0.035672  [ 1300/ 5000]\n",
      "loss: 2.329190  [ 1400/ 5000]\n",
      "loss: 0.625383  [ 1500/ 5000]\n",
      "loss: 0.328945  [ 1600/ 5000]\n",
      "loss: 5.726302  [ 1700/ 5000]\n",
      "loss: 2.229860  [ 1800/ 5000]\n",
      "loss: 0.031787  [ 1900/ 5000]\n",
      "loss: 0.072519  [ 2000/ 5000]\n",
      "loss: 1.761271  [ 2100/ 5000]\n",
      "loss: 17.288715  [ 2200/ 5000]\n",
      "loss: 0.745852  [ 2300/ 5000]\n",
      "loss: 0.048764  [ 2400/ 5000]\n",
      "loss: 0.620363  [ 2500/ 5000]\n",
      "loss: 0.134761  [ 2600/ 5000]\n",
      "loss: 0.603097  [ 2700/ 5000]\n",
      "loss: 0.139035  [ 2800/ 5000]\n",
      "loss: 0.283954  [ 2900/ 5000]\n",
      "loss: 0.598390  [ 3000/ 5000]\n",
      "loss: 0.232712  [ 3100/ 5000]\n",
      "loss: 0.127675  [ 3200/ 5000]\n",
      "loss: 0.104161  [ 3300/ 5000]\n",
      "loss: 0.018104  [ 3400/ 5000]\n",
      "loss: 0.161759  [ 3500/ 5000]\n",
      "loss: 0.031193  [ 3600/ 5000]\n",
      "loss: 0.454306  [ 3700/ 5000]\n",
      "loss: 0.398070  [ 3800/ 5000]\n",
      "loss: 0.195853  [ 3900/ 5000]\n",
      "loss: 0.109760  [ 4000/ 5000]\n",
      "loss: 0.329651  [ 4100/ 5000]\n",
      "loss: 0.123559  [ 4200/ 5000]\n",
      "loss: 0.061790  [ 4300/ 5000]\n",
      "loss: 0.515403  [ 4400/ 5000]\n",
      "loss: 0.331192  [ 4500/ 5000]\n",
      "loss: 0.646782  [ 4600/ 5000]\n",
      "loss: 1.293070  [ 4700/ 5000]\n",
      "loss: 0.227780  [ 4800/ 5000]\n",
      "loss: 0.190045  [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 2340.4%, Avg loss: 3.273485 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.815856  [    0/ 5000]\n",
      "loss: 0.038160  [  100/ 5000]\n",
      "loss: 0.023399  [  200/ 5000]\n",
      "loss: 0.520809  [  300/ 5000]\n",
      "loss: 0.241225  [  400/ 5000]\n",
      "loss: 0.124510  [  500/ 5000]\n",
      "loss: 14.160971  [  600/ 5000]\n",
      "loss: 2.996539  [  700/ 5000]\n",
      "loss: 0.057813  [  800/ 5000]\n",
      "loss: 0.050508  [  900/ 5000]\n",
      "loss: 0.812818  [ 1000/ 5000]\n",
      "loss: 0.226928  [ 1100/ 5000]\n",
      "loss: 0.241093  [ 1200/ 5000]\n",
      "loss: 0.186167  [ 1300/ 5000]\n",
      "loss: 0.198172  [ 1400/ 5000]\n",
      "loss: 0.476831  [ 1500/ 5000]\n",
      "loss: 0.355675  [ 1600/ 5000]\n",
      "loss: 0.500715  [ 1700/ 5000]\n",
      "loss: 10.117385  [ 1800/ 5000]\n",
      "loss: 1.104570  [ 1900/ 5000]\n",
      "loss: 0.163041  [ 2000/ 5000]\n",
      "loss: 6.910456  [ 2100/ 5000]\n",
      "loss: 2.157789  [ 2200/ 5000]\n",
      "loss: 18.260374  [ 2300/ 5000]\n",
      "loss: 0.280778  [ 2400/ 5000]\n",
      "loss: 0.045041  [ 2500/ 5000]\n",
      "loss: 0.137256  [ 2600/ 5000]\n",
      "loss: 0.192086  [ 2700/ 5000]\n",
      "loss: 0.615697  [ 2800/ 5000]\n",
      "loss: 1.057077  [ 2900/ 5000]\n",
      "loss: 0.153109  [ 3000/ 5000]\n",
      "loss: 0.082840  [ 3100/ 5000]\n",
      "loss: 0.038674  [ 3200/ 5000]\n",
      "loss: 0.120609  [ 3300/ 5000]\n",
      "loss: 0.087785  [ 3400/ 5000]\n",
      "loss: 2.166914  [ 3500/ 5000]\n",
      "loss: 1.261844  [ 3600/ 5000]\n",
      "loss: 0.668053  [ 3700/ 5000]\n",
      "loss: 0.212003  [ 3800/ 5000]\n",
      "loss: 0.053517  [ 3900/ 5000]\n",
      "loss: 0.645526  [ 4000/ 5000]\n",
      "loss: 0.170642  [ 4100/ 5000]\n",
      "loss: 0.492684  [ 4200/ 5000]\n",
      "loss: 0.268115  [ 4300/ 5000]\n",
      "loss: 0.131858  [ 4400/ 5000]\n",
      "loss: 0.037448  [ 4500/ 5000]\n",
      "loss: 0.180335  [ 4600/ 5000]\n",
      "loss: 0.062303  [ 4700/ 5000]\n",
      "loss: 1.093976  [ 4800/ 5000]\n",
      "loss: 0.124296  [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 2335.2%, Avg loss: 3.229833 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.020110  [    0/ 5000]\n",
      "loss: 0.551527  [  100/ 5000]\n",
      "loss: 0.150214  [  200/ 5000]\n",
      "loss: 0.078240  [  300/ 5000]\n",
      "loss: 0.030545  [  400/ 5000]\n",
      "loss: 0.049768  [  500/ 5000]\n",
      "loss: 0.292121  [  600/ 5000]\n",
      "loss: 7.993949  [  700/ 5000]\n",
      "loss: 12.961547  [  800/ 5000]\n",
      "loss: 8.648625  [  900/ 5000]\n",
      "loss: 0.094757  [ 1000/ 5000]\n",
      "loss: 41.950825  [ 1100/ 5000]\n",
      "loss: 0.057908  [ 1200/ 5000]\n",
      "loss: 1.452226  [ 1300/ 5000]\n",
      "loss: 0.146000  [ 1400/ 5000]\n",
      "loss: 0.057866  [ 1500/ 5000]\n",
      "loss: 15.032438  [ 1600/ 5000]\n",
      "loss: 0.026406  [ 1700/ 5000]\n",
      "loss: 0.033821  [ 1800/ 5000]\n",
      "loss: 1.946669  [ 1900/ 5000]\n",
      "loss: 13.975533  [ 2000/ 5000]\n",
      "loss: 0.479670  [ 2100/ 5000]\n",
      "loss: 0.934879  [ 2200/ 5000]\n",
      "loss: 0.454735  [ 2300/ 5000]\n",
      "loss: 0.961635  [ 2400/ 5000]\n",
      "loss: 0.119929  [ 2500/ 5000]\n",
      "loss: 0.051478  [ 2600/ 5000]\n",
      "loss: 0.178246  [ 2700/ 5000]\n",
      "loss: 0.591516  [ 2800/ 5000]\n",
      "loss: 14.321856  [ 2900/ 5000]\n",
      "loss: 0.179499  [ 3000/ 5000]\n",
      "loss: 0.435713  [ 3100/ 5000]\n",
      "loss: 4.690902  [ 3200/ 5000]\n",
      "loss: 0.424664  [ 3300/ 5000]\n",
      "loss: 0.050715  [ 3400/ 5000]\n",
      "loss: 14.883666  [ 3500/ 5000]\n",
      "loss: 0.059346  [ 3600/ 5000]\n",
      "loss: 0.077070  [ 3700/ 5000]\n",
      "loss: 3.008440  [ 3800/ 5000]\n",
      "loss: 0.184037  [ 3900/ 5000]\n",
      "loss: 0.097809  [ 4000/ 5000]\n",
      "loss: 3.972870  [ 4100/ 5000]\n",
      "loss: 1.376484  [ 4200/ 5000]\n",
      "loss: 0.043735  [ 4300/ 5000]\n",
      "loss: 9.878272  [ 4400/ 5000]\n",
      "loss: 2.798791  [ 4500/ 5000]\n",
      "loss: 4.220317  [ 4600/ 5000]\n",
      "loss: 0.183756  [ 4700/ 5000]\n",
      "loss: 0.022774  [ 4800/ 5000]\n",
      "loss: 0.110464  [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 2316.9%, Avg loss: 3.182548 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.271948  [    0/ 5000]\n",
      "loss: 10.428435  [  100/ 5000]\n",
      "loss: 0.633677  [  200/ 5000]\n",
      "loss: 0.190013  [  300/ 5000]\n",
      "loss: 0.147056  [  400/ 5000]\n",
      "loss: 0.218932  [  500/ 5000]\n",
      "loss: 0.516065  [  600/ 5000]\n",
      "loss: 0.092088  [  700/ 5000]\n",
      "loss: 0.294659  [  800/ 5000]\n",
      "loss: 0.219018  [  900/ 5000]\n",
      "loss: 6.361643  [ 1000/ 5000]\n",
      "loss: 0.138337  [ 1100/ 5000]\n",
      "loss: 3.053296  [ 1200/ 5000]\n",
      "loss: 0.031717  [ 1300/ 5000]\n",
      "loss: 0.202764  [ 1400/ 5000]\n",
      "loss: 12.880126  [ 1500/ 5000]\n",
      "loss: 0.125969  [ 1600/ 5000]\n",
      "loss: 13.447280  [ 1700/ 5000]\n",
      "loss: 0.306266  [ 1800/ 5000]\n",
      "loss: 0.029078  [ 1900/ 5000]\n",
      "loss: 0.252830  [ 2000/ 5000]\n",
      "loss: 0.577115  [ 2100/ 5000]\n",
      "loss: 43.316002  [ 2200/ 5000]\n",
      "loss: 0.020102  [ 2300/ 5000]\n",
      "loss: 6.065883  [ 2400/ 5000]\n",
      "loss: 0.462704  [ 2500/ 5000]\n",
      "loss: 0.017521  [ 2600/ 5000]\n",
      "loss: 0.060313  [ 2700/ 5000]\n",
      "loss: 0.212207  [ 2800/ 5000]\n",
      "loss: 0.034308  [ 2900/ 5000]\n",
      "loss: 3.941809  [ 3000/ 5000]\n",
      "loss: 0.034662  [ 3100/ 5000]\n",
      "loss: 0.032443  [ 3200/ 5000]\n",
      "loss: 0.332415  [ 3300/ 5000]\n",
      "loss: 0.109781  [ 3400/ 5000]\n",
      "loss: 0.050022  [ 3500/ 5000]\n",
      "loss: 0.051059  [ 3600/ 5000]\n",
      "loss: 0.153611  [ 3700/ 5000]\n",
      "loss: 0.032171  [ 3800/ 5000]\n",
      "loss: 2.123932  [ 3900/ 5000]\n",
      "loss: 0.101235  [ 4000/ 5000]\n",
      "loss: 0.109763  [ 4100/ 5000]\n",
      "loss: 0.985070  [ 4200/ 5000]\n",
      "loss: 122.974052  [ 4300/ 5000]\n",
      "loss: 0.017875  [ 4400/ 5000]\n",
      "loss: 0.179218  [ 4500/ 5000]\n",
      "loss: 0.563242  [ 4600/ 5000]\n",
      "loss: 2.270025  [ 4700/ 5000]\n",
      "loss: 2.889316  [ 4800/ 5000]\n",
      "loss: 0.770132  [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 2314.7%, Avg loss: 3.158319 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.121795  [    0/ 5000]\n",
      "loss: 0.018996  [  100/ 5000]\n",
      "loss: 0.195482  [  200/ 5000]\n",
      "loss: 0.199830  [  300/ 5000]\n",
      "loss: 0.935924  [  400/ 5000]\n",
      "loss: 0.777680  [  500/ 5000]\n",
      "loss: 0.142678  [  600/ 5000]\n",
      "loss: 7.845778  [  700/ 5000]\n",
      "loss: 106.673042  [  800/ 5000]\n",
      "loss: 0.088483  [  900/ 5000]\n",
      "loss: 0.100334  [ 1000/ 5000]\n",
      "loss: 49.589161  [ 1100/ 5000]\n",
      "loss: 0.018582  [ 1200/ 5000]\n",
      "loss: 0.018278  [ 1300/ 5000]\n",
      "loss: 1.554809  [ 1400/ 5000]\n",
      "loss: 18.880804  [ 1500/ 5000]\n",
      "loss: 0.660509  [ 1600/ 5000]\n",
      "loss: 0.725066  [ 1700/ 5000]\n",
      "loss: 0.110270  [ 1800/ 5000]\n",
      "loss: 0.147115  [ 1900/ 5000]\n",
      "loss: 0.022539  [ 2000/ 5000]\n",
      "loss: 0.426696  [ 2100/ 5000]\n",
      "loss: 0.090919  [ 2200/ 5000]\n",
      "loss: 0.206926  [ 2300/ 5000]\n",
      "loss: 23.360945  [ 2400/ 5000]\n",
      "loss: 2.194538  [ 2500/ 5000]\n",
      "loss: 0.438838  [ 2600/ 5000]\n",
      "loss: 0.052639  [ 2700/ 5000]\n",
      "loss: 0.499944  [ 2800/ 5000]\n",
      "loss: 0.255067  [ 2900/ 5000]\n",
      "loss: 0.202410  [ 3000/ 5000]\n",
      "loss: 0.027410  [ 3100/ 5000]\n",
      "loss: 13.940326  [ 3200/ 5000]\n",
      "loss: 33.107773  [ 3300/ 5000]\n",
      "loss: 8.147186  [ 3400/ 5000]\n",
      "loss: 0.562328  [ 3500/ 5000]\n",
      "loss: 4.407710  [ 3600/ 5000]\n",
      "loss: 0.028655  [ 3700/ 5000]\n",
      "loss: 15.354883  [ 3800/ 5000]\n",
      "loss: 2.893965  [ 3900/ 5000]\n",
      "loss: 0.333659  [ 4000/ 5000]\n",
      "loss: 1.216882  [ 4100/ 5000]\n",
      "loss: 0.188118  [ 4200/ 5000]\n",
      "loss: 27.855164  [ 4300/ 5000]\n",
      "loss: 0.927689  [ 4400/ 5000]\n",
      "loss: 42.765770  [ 4500/ 5000]\n",
      "loss: 0.567450  [ 4600/ 5000]\n",
      "loss: 0.167873  [ 4700/ 5000]\n",
      "loss: 0.031458  [ 4800/ 5000]\n",
      "loss: 39.542950  [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 2331.0%, Avg loss: 3.101799 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_x = test_x.to(device)\n",
    "test_y = test_y.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(test_x)\n",
    "\n",
    "pred = torch.squeeze(pred)\n",
    "pred = torch.squeeze(pred)\n",
    "\n",
    "pred = pred.cpu() # Para transformar em numpy, vê-se necessário passar a variável para o cpu\n",
    "\n",
    "y = pred.numpy()\n",
    "\n",
    "print(y.size)\n",
    "\n",
    "dt = 0.1\n",
    "max_time = 30\n",
    "tempo = np.arange(0, max_time, dt)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(tempo, y, 'k-')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch-cuda11.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ecf4855ac3054153cb32a566375621934de42d4bb3f19a1ac18025b10a022b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
